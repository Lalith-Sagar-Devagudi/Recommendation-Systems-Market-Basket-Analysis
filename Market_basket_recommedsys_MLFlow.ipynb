{"cells":[{"cell_type":"markdown","source":["## Installation of PySpark and MLflow\n\nThis code cell installs the necessary packages, PySpark and MLflow, using the `%pip` magic command in Jupyter Notebook.\n\nThe first command, `%pip install pyspark`, installs the PySpark package. PySpark is the Python library for Apache Spark, a fast and general-purpose cluster computing system. It provides high-level APIs for distributed data processing, machine learning, and graph processing.\n\nThe second command, `%pip install mlflow`, installs the MLflow package. MLflow is an open-source platform for managing the machine learning lifecycle. It offers tracking and versioning of experiments, packaging of models, and deployment capabilities.\n\nBy executing these commands, you ensure that both PySpark and MLflow are installed in the environment, allowing you to use their functionalities for data processing and machine learning tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3b39531d-0a8c-4912-b1d4-9aa1e60b6dca","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install pyspark\n%pip install mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"32c2de33-f28a-452c-9f3b-19500060cc39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nCollecting pyspark\n  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\nCollecting py4j==0.10.9.7\n  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status 'done'\n  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317143 sha256=9ce1244fdab02d217089622f734f0fe05321fb1d0809bf055008b2a16e313bfc\n  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9.7 pyspark-3.4.0\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-2.4.1-py3-none-any.whl (18.1 MB)\nCollecting gunicorn<21\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nCollecting pyyaml<7,>=5.1\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\nCollecting gitpython<4,>=2.1.0\n  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\nCollecting cloudpickle<3\n  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nCollecting alembic!=1.10.0,<2\n  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nCollecting Flask<3\n  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\nRequirement already satisfied: pyarrow<13,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nCollecting markdown<4,>=3.3\n  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting importlib-metadata!=4.7.0,<7,>=3.7.0\n  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\nCollecting sqlalchemy<3,>=1.4.0\n  Downloading SQLAlchemy-2.0.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nCollecting sqlparse<1,>=0.4.0\n  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\nCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nCollecting Mako\n  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\nCollecting pyjwt>=1.7.0\n  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\nCollecting oauthlib>=3.1.0\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting tabulate>=0.7.7\n  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: urllib3<2.0.0,>=1.26.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.9)\nCollecting websocket-client>=0.32.0\n  Downloading websocket_client-1.5.3-py3-none-any.whl (56 kB)\nCollecting Werkzeug>=2.3.3\n  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\nCollecting click<9,>=7.0\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\nCollecting itsdangerous>=2.1.2\n  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\nCollecting blinker>=1.6.2\n  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\nCollecting Jinja2<4,>=2.11\n  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\nCollecting zipp>=0.5\n  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\nCollecting typing-extensions>=4\n  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nCollecting MarkupSafe>=2.0\n  Downloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nBuilding wheels for collected packages: databricks-cli\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status 'done'\n  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143878 sha256=816cd5e9b0a539542955e8050e13f285fbfed3ec77b25ed755e8f57adc34157e\n  Stored in directory: /root/.cache/pip/wheels/b6/90/68/94d223a35a3910c1512a8d42d9f8333ce567ef26e250a56227\nSuccessfully built databricks-cli\nInstalling collected packages: zipp, typing-extensions, smmap, MarkupSafe, greenlet, Werkzeug, websocket-client, tabulate, sqlalchemy, pyjwt, oauthlib, Mako, Jinja2, itsdangerous, importlib-metadata, gitdb, click, blinker, sqlparse, querystring-parser, pyyaml, markdown, gunicorn, gitpython, Flask, docker, databricks-cli, cloudpickle, alembic, mlflow\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n    Can't uninstall 'click'. No files were found to uninstall.\nSuccessfully installed Flask-2.3.2 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.3 Werkzeug-2.3.6 alembic-1.11.1 blinker-1.6.2 click-8.1.3 cloudpickle-2.2.1 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.31 greenlet-2.0.2 gunicorn-20.1.0 importlib-metadata-6.6.0 itsdangerous-2.1.2 markdown-3.4.3 mlflow-2.4.1 oauthlib-3.2.2 pyjwt-2.7.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 sqlalchemy-2.0.16 sqlparse-0.4.4 tabulate-0.9.0 typing-extensions-4.6.3 websocket-client-1.5.3 zipp-3.15.0\nPython interpreter will be restarted.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Importing Required Libraries\n\nIn this code cell, we import several libraries that are necessary for the subsequent code execution. \n\n- `pyspark` is imported from `SparkContext`, `functions`, `SparkSession`, and `Column` modules. These modules provide essential functionalities for working with Spark and Spark SQL.\n\n- `FPGrowth` is imported from `pyspark.ml.fpm` module. It is a class that implements the FP-Growth algorithm for frequent pattern mining.\n\n- `mlflow` and `mlflow.spark` are imported to utilize the MLflow library. MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, and these imports enable MLflow integration with Spark.\n\nBy importing these libraries, we ensure that the required functionality and classes are available for performing the subsequent data processing and modeling tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"beb661ac-7086-4b60-b0db-57a8db80b332","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark import SparkContext\nfrom pyspark.sql import functions as f, SparkSession, Column\nfrom pyspark.ml.fpm import FPGrowth\nimport mlflow\nimport mlflow.spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b28845e8-ba5b-4570-a636-800aa50411bf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Creating a Spark Session\n\nIn this code cell, a Spark Session is created using the `SparkSession.builder` object. A Spark Session is the entry point for working with structured data in Spark and provides a programming interface to interact with various Spark functionalities.\n\nThe `appName` parameter is set to \"MarketbasketMLFlow\", which specifies the name of the Spark application. This name helps identify the application in the Spark cluster.\n\nThe `getOrCreate()` method is called on the `SparkSession.builder` object to either retrieve an existing Spark Session or create a new one if it doesn't exist. This ensures that only one Spark Session is created per application.\n\nBy creating a Spark Session, we establish a connection to the Spark cluster and enable the execution of Spark operations on distributed data. The Spark Session provides a unified interface for working with structured data, including DataFrame and SQL operations, machine learning, and streaming capabilities."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"94db1e81-cf81-46de-8a5e-848d0d32f6f3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"MarketbasketMLFlow\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e801ef8e-a0de-4db9-b6d7-119a943c156d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Starting an MLflow Run\n\nIn this code cell, an MLflow run is started using the `mlflow.start_run()` function. MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, and it enables tracking of experiments, parameters, metrics, and artifacts.\n\nWithin the MLflow run, the following steps are performed:\n\n1. Logging Spark Version: The Spark version is logged using the `mlflow.log_param()` function. This captures the version of Spark being used in the experiment.\n\n2. Reading the Data: Two CSV files, \"basket.csv\" and \"Groceries_data.csv\", are read into Spark DataFrames. The data is loaded using the `spark.read.csv()` function, and column names are assumed to be present in the first row (header=True). Additionally, a new column \"id\" is added to each DataFrame using the `f.monotonically_increasing_id()` function.\n\n3. Logging Data Paths: The paths of the input CSV files are logged using the `mlflow.log_param()` function. This provides a record of the data sources used in the experiment.\n\n4. Computing the Number of Baskets: The DataFrame `df_all` is grouped by the \"Member_number\" column, and the count of baskets for each member is computed. This information is stored in the `num_baskets` variable.\n\n5. Logging the Number of Baskets: The count of baskets is logged as a metric using the `mlflow.log_metric()` function. This metric represents the total number of baskets in the dataset.\n\n6. Removing Null Values: The DataFrame `df` is transformed to remove null values. The \"basket\" column is first selected, and then the null values within the array are removed using `f.array_except()` function.\n\n7. Performing Market Basket Analysis: The FP-Growth algorithm is applied to the aggregated DataFrame `df_aggregated` using the `FPGrowth` class from `pyspark.ml.fpm`. The minimum support and minimum confidence thresholds are set to 0.001. The resulting model is stored in the `model` variable.\n\n8. Logging the Model: The trained model is logged using the `mlflow.spark.log_model()` function. This saves the model artifacts to be later retrieved and used for predictions.\n\n9. Retrieving the Run ID: The run ID of the active MLflow run is obtained using `mlflow.active_run().info.run_id` and stored in the `run_id` variable. This run ID can be used to reference this specific MLflow run for later analysis or retrieval of logged information.\n\nThe code cell performs various steps to log information, compute market basket analysis, and save the model using MLflow, allowing for better experimentation tracking and model management."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"28ecde34-576b-470b-8c7c-fa878dcd11b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Start an MLflow run\nwith mlflow.start_run():\n    # Log the Spark version\n    mlflow.log_param(\"spark_version\", spark.version)\n\n    # Read the data\n    df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/lalithsagardevagudi@gmail.com/basket.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\n    df_all = spark.read.csv(\"dbfs:/FileStore/shared_uploads/lalithsagardevagudi@gmail.com/Groceries_data.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\n\n    # Log the data paths\n    mlflow.log_param(\"basket_data_path\", \"dbfs:/FileStore/shared_uploads/lalithsagardevagudi@gmail.com/basket.csv\")\n    mlflow.log_param(\"groceries_data_path\", \"dbfs:/FileStore/shared_uploads/lalithsagardevagudi@gmail.com/Groceries_data.csv\")\n\n    # Compute number of baskets\n    num_baskets = df_all.groupBy(\"Member_number\").count()\n\n    # Log the number of baskets\n    mlflow.log_metric(\"num_baskets\", num_baskets.count())\n\n    # Remove nulls\n    df_basket = df.select(\"id\", f.array([df[c] for c in df.columns[:11]]).alias(\"basket\"))\n    df_aggregated = df_basket.select(\"id\", f.array_except(\"basket\", f.array(f.lit(None))).alias(\"basket\"))\n\n    # Perform market basket analysis\n    fp_growth = FPGrowth(minSupport=0.001, minConfidence=0.001, itemsCol='basket', predictionCol='prediction')\n    model = fp_growth.fit(df_aggregated)\n\n    # Log the model\n    mlflow.spark.log_model(model, \"market_basket_model\")\n\n    # Retrieve the run ID\n    run_id = mlflow.active_run().info.run_id\n    # print(\"MLflow run ID:\", run_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cee48823-1133-4f74-82bd-bc1cedc3c5e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["2023/06/12 17:55:09 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Loading the Trained Model and Generating Predictions\n\nIn this code cell, the trained model is loaded using MLflow and used to generate predictions on new data. The following steps are performed:\n\n1. Model Loading: The `model_uri` variable is set to the URI of the trained model artifact. The URI is constructed using the `run_id` obtained earlier in the code. This URI specifies the location of the model within MLflow. The `mlflow.spark.load_model()` function is then called to load the model into the `model` variable.\n\n2. Obtaining the FPGrowthModel: The FPGrowthModel from the loaded model is extracted. It is assumed that the FPGrowthModel is the last stage in the pipeline, so `model.stages[-1]` is used to access it. The FPGrowthModel is stored in the `fpgrowth_model` variable.\n\n3. Creating a PySpark DataFrame: A new PySpark DataFrame called `new_df` is created to hold the new data for which predictions will be generated. The DataFrame consists of a single column named 'basket', and it contains two rows of new data: (['beef'],) and (['oil'],). The `spark.sparkContext.parallelize()` function is used to parallelize the new data, and the resulting RDD is converted to a DataFrame using the `toDF()` function. The column names are specified using the `columns` list.\n\n4. Generating Predictions: The `model.transform()` function is used to generate predictions on the `new_df` DataFrame. The `model` variable contains the loaded model, and the `transform()` function applies the model to the input DataFrame. The predictions are stored in the `predictions` DataFrame.\n\n5. Showing the Recommendations: The `predictions.show(5)` statement is used to display the top 5 rows of the predictions DataFrame, which includes the generated recommendations based on the input data.\n\nBy loading the trained model and generating predictions, this code cell demonstrates how to use the trained model to make recommendations for new data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"58dcb2b6-2250-407c-bf79-915b2d401921","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Load the trained model\nmodel_uri = \"runs:/\"+run_id+\"/market_basket_model\"  # Replace <run-id> with the actual run ID of the model\nmodel = mlflow.spark.load_model(model_uri)\n\n# Get the FPGrowthModel from the pipeline model\nfpgrowth_model = model.stages[-1]  # Assuming the FPGrowthModel is the last stage in the pipeline\n\n# Create a PySpark DataFrame with new data\ncolumns = ['basket']\nnew_data = [(['beef'],), (['oil'],)]\nrdd = spark.sparkContext.parallelize(new_data)\nnew_df = rdd.toDF(columns)\n\n# Generate predictions using the model\npredictions = model.transform(new_df)\n\n# Show the recommendations\npredictions.show(5)\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"11db1de6-d496-4c8a-9b8e-ab863e2ae466","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["2023/06/12 17:56:07 INFO mlflow.spark: 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model' resolved as 'dbfs:/databricks/mlflow-tracking/1564150963134392/5af8033cf02a4c9aa777b9a67284c37e/artifacts/market_basket_model'\n2023/06/12 17:56:11 INFO mlflow.spark: URI 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model/sparkml' does not point to the current DFS.\n2023/06/12 17:56:11 INFO mlflow.spark: File 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model/sparkml' not found on DFS. Will attempt to upload the file.\n2023/06/12 17:56:13 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/c8f5da9a-44c2-45d3-b873-801c0845a3c7\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------------------+\n|basket|          prediction|\n+------+--------------------+\n|[beef]|[frankfurter, rol...|\n| [oil]|[rolls/buns, yogu...|\n+------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Evaluating Predictions using Lift\n\nThis code cell demonstrates how to evaluate the predictions generated by the FPGrowth model using the concept of lift. The steps involved are as follows:\n\n1. Association Rules: The association rules learned by the FPGrowth model are stored in the `association_rules` variable. These rules represent the relationships between items in the dataset.\n\n2. Joining Predictions and Association Rules: The predictions generated earlier are joined with the association rules based on the common items. This is done using the `join()` function, where the join condition is specified as `association_rules.antecedent == predictions.basket`. The result is stored in the `eval_df` DataFrame.\n\n3. Calculating Lift: The lift is a measure of the strength of an association rule. In this code, the calculation of lift is commented out. The lift can be calculated by dividing the confidence of each association rule by the overall confidence of the rules. You can uncomment the relevant line of code (`eval_df = eval_df.withColumn(\"lift\", col(\"confidence\") / association_rules.select(\"confidence\").first())`) to calculate the lift for each association rule.\n\n4. Showing the Evaluation Results: Finally, the `eval_df.show()` statement is used to display the evaluation results. This will show the joined DataFrame with the association rules and the corresponding predictions. If you have uncommented the lift calculation, the lift values will also be included in the evaluation results.\n\nBy evaluating the predictions using lift and examining the association rules, you can gain insights into the strength and significance of the relationships between different items in the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"32d9203e-1511-4d82-a15f-9d87cd4b01d2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Evaluate the predictions using lift\nassociation_rules = fpgrowth_model.associationRules\n\n# Join the association rules with the predictions based on the common items\neval_df = association_rules.join(predictions, association_rules.antecedent == predictions.basket, \"inner\")\n\n# Calculate the lift for each association rule\n# eval_df = eval_df.withColumn(\"lift\", col(\"confidence\") / association_rules.select(\"confidence\").first())\n\n# Show the evaluation results\neval_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"98180dc4-6bbf-4171-bde0-da6cadf2f7ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\n|antecedent|          consequent|          confidence|               lift|             support|basket|          prediction|\n+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\n|    [beef]|              [curd]| 0.03740157480314961|  1.110396356705412|0.001269798837131...|[beef]|[frankfurter, rol...|\n|    [beef]| [frozen vegetables]| 0.03740157480314961| 1.3356557608103283|0.001269798837131...|[beef]|[frankfurter, rol...|\n|    [beef]|         [margarine]| 0.04133858267716536| 1.2832971215734963|0.001403461872619127|[beef]|[frankfurter, rol...|\n|    [beef]|        [whole milk]|  0.1377952755905512| 0.8725479088706803|0.004678206242063757|[beef]|[frankfurter, rol...|\n|    [beef]|[whipped/sour cream]| 0.04133858267716536| 0.9457939030556961|0.001403461872619127|[beef]|[frankfurter, rol...|\n|    [beef]|    [tropical fruit]| 0.03346456692913386|0.49381687865939844|0.001136135801644...|[beef]|[frankfurter, rol...|\n|    [beef]|            [butter]| 0.03346456692913386| 0.9501524003048006|0.001136135801644...|[beef]|[frankfurter, rol...|\n|    [beef]|              [soda]|  0.0531496062992126| 0.5473348651446099|0.001804450979081735|[beef]|[frankfurter, rol...|\n|    [beef]|     [shopping bags]| 0.03740157480314961| 0.7860109041847299|0.001269798837131...|[beef]|[frankfurter, rol...|\n|    [beef]|      [citrus fruit]|  0.0531496062992126|  1.000349130886941|0.001804450979081735|[beef]|[frankfurter, rol...|\n|    [beef]|       [brown bread]|0.045275590551181105| 1.2033013524286376|0.001537124908106...|[beef]|[frankfurter, rol...|\n|    [beef]|  [other vegetables]| 0.08267716535433071| 0.6771201013666396|0.002806923745238254|[beef]|[frankfurter, rol...|\n|    [beef]|            [pastry]| 0.03543307086614173| 0.6849935909174144|0.001202967319387...|[beef]|[frankfurter, rol...|\n|    [beef]|     [bottled water]| 0.03937007874015748| 0.6487824759790488|0.001336630354875...|[beef]|[frankfurter, rol...|\n|    [beef]|        [newspapers]| 0.04921259842519685| 1.2652373028113755|0.001670787943594199|[beef]|[frankfurter, rol...|\n|    [beef]|[fruit/vegetable ...|0.031496062992125984| 0.9258852466624384|0.001069304283900...|[beef]|[frankfurter, rol...|\n|    [beef]|            [yogurt]| 0.06496062992125984| 0.7564248291920708|0.002205440085544343|[beef]|[frankfurter, rol...|\n|    [beef]|       [canned beer]| 0.02952755905511811| 0.6293744531933508|0.001002472766156...|[beef]|[frankfurter, rol...|\n|    [beef]|      [bottled beer]|0.031496062992125984| 0.6950967412259308|0.001069304283900...|[beef]|[frankfurter, rol...|\n|    [beef]|   [root vegetables]| 0.04921259842519685| 0.7073661001308554|0.001670787943594199|[beef]|[frankfurter, rol...|\n+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Generating Recommendations based on Association Rules\n\nIn this code cell, we demonstrate how to generate recommendations based on the association rules learned by the FPGrowth model. Here are the steps involved:\n\n1. Specify the Input Item: You need to specify the item for which you want to generate recommendations. In the provided code, the variable `input_item` is set to \"beef\". You can replace it with your desired input item.\n\n2. Filtering the Association Rules: The association rules learned by the FPGrowth model are filtered based on the condition that the antecedent (left-hand side) of the rule contains the input item. This is done using the `fpgrowth_model.associationRules.filter()` function. The filtered rules are stored in the `recommendations` variable.\n\n3. Ordering the Recommendations: The filtered rules are then ordered by confidence, lift, and support in descending order using the `orderBy()` function. This ensures that the recommendations are ranked based on these criteria.\n\n4. Showing the Recommendations: Finally, the `recommendations.show()` statement is used to display the recommendations. This will show the association rules that contain the input item, ordered by confidence, lift, and support.\n\nBy following these steps, you can generate recommendations based on the association rules and gain insights into the items that are likely to be associated with the input item."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"64fee9b1-ca56-4010-8d68-c5d46a7033b2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import mlflow.pyfunc\nfrom pyspark.sql import functions as F\n\n# Specify the input item for which you want to make recommendations\ninput_item = \"beef\"  # Replace with your desired input item\n\n# Generate recommendations using the association rules\nrecommendations = fpgrowth_model.associationRules.filter(F.array_contains(fpgrowth_model.associationRules.antecedent, input_item))\n\n# Order the filtered rules by confidence, lift, and support in descending order\nrecommendations = recommendations.orderBy(\n    F.desc(\"confidence\"),\n    F.desc(\"lift\"),\n    F.desc(\"support\")\n)\n\n\n# Show the recommendations\nrecommendations.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e9be490a-cc68-4c8e-a787-b3226730e586","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+--------------------+--------------------+-------------------+--------------------+\n|antecedent|          consequent|          confidence|               lift|             support|\n+----------+--------------------+--------------------+-------------------+--------------------+\n|    [beef]|        [whole milk]|  0.1377952755905512| 0.8725479088706803|0.004678206242063757|\n|    [beef]|  [other vegetables]| 0.08267716535433071| 0.6771201013666396|0.002806923745238254|\n|    [beef]|            [yogurt]| 0.06496062992125984| 0.7564248291920708|0.002205440085544343|\n|    [beef]|      [citrus fruit]|  0.0531496062992126|  1.000349130886941|0.001804450979081735|\n|    [beef]|              [soda]|  0.0531496062992126| 0.5473348651446099|0.001804450979081735|\n|    [beef]|        [newspapers]| 0.04921259842519685| 1.2652373028113755|0.001670787943594199|\n|    [beef]|   [root vegetables]| 0.04921259842519685| 0.7073661001308554|0.001670787943594199|\n|    [beef]|        [rolls/buns]|0.047244094488188976| 0.4294735029324251|0.001603956425850431|\n|    [beef]|       [brown bread]|0.045275590551181105| 1.2033013524286376|0.001537124908106...|\n|    [beef]|         [margarine]| 0.04133858267716536| 1.2832971215734963|0.001403461872619127|\n|    [beef]|[whipped/sour cream]| 0.04133858267716536| 0.9457939030556961|0.001403461872619127|\n|    [beef]|     [bottled water]| 0.03937007874015748| 0.6487824759790488|0.001336630354875...|\n|    [beef]| [frozen vegetables]| 0.03740157480314961| 1.3356557608103283|0.001269798837131...|\n|    [beef]|              [curd]| 0.03740157480314961|  1.110396356705412|0.001269798837131...|\n|    [beef]|     [shopping bags]| 0.03740157480314961| 0.7860109041847299|0.001269798837131...|\n|    [beef]|            [pastry]| 0.03543307086614173| 0.6849935909174144|0.001202967319387...|\n|    [beef]|            [butter]| 0.03346456692913386| 0.9501524003048006|0.001136135801644...|\n|    [beef]|     [domestic eggs]| 0.03346456692913386| 0.9022167837128469|0.001136135801644...|\n|    [beef]|    [tropical fruit]| 0.03346456692913386|0.49381687865939844|0.001136135801644...|\n|    [beef]|[fruit/vegetable ...|0.031496062992125984| 0.9258852466624384|0.001069304283900...|\n+----------+--------------------+--------------------+-------------------+--------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Stop the Spark session\nspark.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4afea05c-6c00-47ba-998e-0f0da7626c64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Market_basket_recommedsys_MLFlow","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
